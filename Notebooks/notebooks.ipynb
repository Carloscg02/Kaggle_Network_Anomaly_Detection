{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f75b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db358ba8",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69748112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks\n",
    "\n",
    "# ROOT apunta a la carpeta raíz del proyecto\n",
    "ROOT = Path().resolve().parent\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "# 2️⃣ Importar funciones desde src\n",
    "from src.dataset import load_data\n",
    "\n",
    "# 3️⃣ Cargar datos\n",
    "df = load_data()\n",
    "\n",
    "# 4️⃣ Mostrar las primeras filas\n",
    "\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_by_rows(df, n_test):\n",
    "    if n_test <= 0 or n_test >= len(df):\n",
    "        raise ValueError(\"n_test debe ser mayor que 0 y menor que el número de filas del DataFrame\")\n",
    "    \n",
    "    test_df = df.tail(n_test)\n",
    "    train_df = df.iloc[:-n_test]\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3796fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = split_train_test_by_rows(df, 9158)\n",
    "num_cols = train_df.columns[2:-1]  \n",
    "train_df.loc[:, num_cols] = train_df.loc[:, num_cols].apply(pd.to_numeric, errors='coerce')\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fc1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc059a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61d2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de observaciones duplicadas\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a026a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuneamos los colores para que sean más intuitivos\n",
    "custom_palette = {\n",
    "    0: \"grey\",\n",
    "    1: \"red\"\n",
    "}\n",
    "\n",
    "# Pairplot:\n",
    "# Funcion util (cuando tengamos pocas variables) para entender las relaciones entre las covariables con la respuesta y entre las propias variables\n",
    "\n",
    "sns.pairplot(train_df[['Time', 'CellName', 'PRBUsageUL', 'PRBUsageDL', 'meanThr_DL',\n",
    "       'meanThr_UL', 'maxThr_DL', 'maxThr_UL', 'meanUE_DL', 'meanUE_UL',\n",
    "       'maxUE_DL', 'maxUE_UL', 'maxUE_UL+DL', 'Unusual']], hue = \"Unusual\", palette=custom_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "(df['Unusual']).hist()\n",
    "\n",
    "df['Unusual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb73dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORRELACIÓN\n",
    "corr_matrix = train_df.iloc[:,2:-1].corr()\n",
    "print(corr_matrix)\n",
    "sns.heatmap(abs(train_df.iloc[:,2:-1].corr()), cmap = \"rocket_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7288f210",
   "metadata": {},
   "source": [
    "## Analisis de correlación\n",
    "\n",
    "-maxUE_DL, maxUE_DL y maxUE_UL+DL muy correlados\n",
    "\n",
    "-maxUE_DL, maxUE_DL y maxUE_UL+DL muy correlados con PRBusageUL (sobretodo) y también con PRBusageDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666abe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[:,:-1].plot(kind='box', subplots=True, layout=(4, 3), figsize=(12, 8), sharex=False, sharey=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista con las observaciones que son un outilier en alguna de las covariables\n",
    "mask = pd.Series(True, index=df.index)\n",
    "\n",
    "for col in train_df.columns[2:-1]:\n",
    "    # Cuartiles\n",
    "    Q1 = train_df[col].quantile(0.25)\n",
    "    Q3 = train_df[col].quantile(0.75)\n",
    "\n",
    "    # Rango intercuartilico\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Actualizamos mask: True solo si el valor está dentro de los límites\n",
    "    mask &= train_df[col].between(lower_bound, upper_bound)\n",
    "\n",
    "# Observaciones originales\n",
    "print(f\"Número de filas originales: {len(train_df)}\")\n",
    "\n",
    "# Aplicamos mask para eliminar filas con al menos un outlier\n",
    "train_df_sinOutliers = train_df[mask]\n",
    "\n",
    "# Resultado tras eliminar outliers\n",
    "print(f\"Número de filas sin outliers: {len(train_df_sinOutliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1464ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_sinOutliers.iloc[:,:-1].plot(kind='box', subplots=True, layout=(4, 3), figsize=(12, 8), sharex=False, sharey=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5beece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "# Calculamos la asimetría\n",
    "skewness = train_df_sinOutliers.select_dtypes(include='number').apply(skew)\n",
    "\n",
    "# Ordenamos las variables por asimetría\n",
    "print(skewness.sort_values(ascending=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb87781",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_df_sinOutliers.columns[2:-2]:\n",
    "    # Calculamos la skewness\n",
    "    sk = skew(train_df_sinOutliers[col])\n",
    "    if sk > 1:\n",
    "        train_df_sinOutliers[col] = np.log1p(train_df_sinOutliers[col])  # log(x + 1) para evitar log(0)\n",
    "    elif sk < -1:\n",
    "        max_val = train_df_sinOutliers[col].max()\n",
    "        train_df_sinOutliers[col] = np.log1p(max_val + 1 - train_df_sinOutliers[col])  # reflejar + log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c64ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "# Calculamos la asimetría\n",
    "skewness = train_df_sinOutliers.select_dtypes(include='number').apply(skew)\n",
    "\n",
    "# Ordenamos las variables por asimetría\n",
    "print(skewness.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c97bb",
   "metadata": {},
   "source": [
    "### Situación actual:\n",
    "train_df_sinOutliers -> train dataset, sin outliers y con asimetrías transformadas. \n",
    "\n",
    "train_df -> train dataset sin procesar\n",
    "\n",
    "*Comprobar con que modelos se comporta mejor o peor cada uno, con los más simples (regresión logistica, KNN debería funcionar mejor el transformado. Puede ser que el completo de buenos resultados en modelos complejos capaces de captar ese tipo de patrones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_sinOutliers.iloc[:,:-1].plot(kind='box', subplots=True, layout=(4, 3), figsize=(12, 8), sharex=False, sharey=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f4b0e",
   "metadata": {},
   "source": [
    "# ENTRENAMIENTO MODELOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834df5b",
   "metadata": {},
   "source": [
    "## Escalado z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(train_df.iloc[:,2:-1])\n",
    "\n",
    "df_escalado = train_df\n",
    "\n",
    "df_escalado.iloc[:,2:-1] = scaler.transform(df_escalado.iloc[:,2:-1])\n",
    "\n",
    "df_escalado_procesado = train_df_sinOutliers\n",
    "\n",
    "df_escalado_procesado.iloc[:,2:-1] = scaler.transform(df_escalado_procesado.iloc[:,2:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256865d3",
   "metadata": {},
   "source": [
    "### escalados\n",
    "\n",
    "df_escalado_procesado -> sin outliers y simetrico\n",
    "\n",
    "df_escalado -> original sin procesar pero si escalado\n",
    "\n",
    "test_df_escalado -> test escalado (con scaler del train, para no contaminar con datos de test, y sin procesar ni limpiar ouliers) (NO tengo las etiquetas del test, la competición kaggle está cerrada. Usaré el test de train para comprobar la precisión, sacando subconjunto de validación)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4aae7a",
   "metadata": {},
   "source": [
    "## Modelo regresión logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Sin preprocesar (con outliers y distribuciones asimetricas)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_escalado.drop(['Time', 'CellName','Unusual'],axis=1),\n",
    "                                                    df_escalado['Unusual'], test_size=0.30,\n",
    "                                                    random_state=101)\n",
    "\n",
    "filas_con_nan = X_train[X_train.isna().any(axis=1)]\n",
    "\n",
    "# Mostrar las filas con NaN\n",
    "print(f\"Filas con NaN en X_train ({len(filas_con_nan)} de {len(X_train)}):\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9056d4",
   "metadata": {},
   "source": [
    "Tenemos 67 de 25832 filas con valores nulos en maxUE. Se pueden eliminar o imputar.\n",
    "\n",
    "1. Eliminando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar NaN\n",
    "X_train_sin_Nan = X_train.dropna()\n",
    "y_train_sin_Nan = y_train.loc[X_train_sin_Nan.index]\n",
    "X_val_sin_Nan = X_val.dropna()\n",
    "y_val_sin_Nan = y_val.loc[X_val_sin_Nan.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f88598",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train['maxUE_DL'].mean())\n",
    "print(X_train['maxUE_UL'].mean())\n",
    "print(X_train['maxUE_UL+DL'].mean())\n",
    "\n",
    "#de momento eliminamos y ya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288145c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logmodel = LogisticRegression()\n",
    "\n",
    "logmodel.fit(X_train_sin_Nan,y_train_sin_Nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4ed9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logmodel.predict(X_val_sin_Nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da1a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_val_sin_Nan,predictions))\n",
    "\n",
    "print(confusion_matrix(y_val_sin_Nan,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee85c438",
   "metadata": {},
   "source": [
    "Regresión logistica, con los datos sin preprocesar es malo, con un F1 de 0.42. El modelo está clasificando como 0 (comportamiento normal) con demasiada frecuencia, tiene 0.00423 de recall para la clase 1, es decir, de las que son 1 solo clasifica bien 0.04%. \n",
    "El modelo Ha sobreajustado a la clase 0 al ser mucho más numerosa y no añadir ninguna tecnica de control. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283622e4",
   "metadata": {},
   "source": [
    "## Sin outliers y con transofrmaciones para simetría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ab1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocesando\n",
    "\n",
    "X_train_preprocesado, X_val_preprocesado, y_train, y_val = train_test_split(df_escalado_procesado.drop(['Time', 'CellName','Unusual'],axis=1),\n",
    "                                                    df_escalado_procesado['Unusual'], test_size=0.30,\n",
    "                                                    random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d80800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar NaN\n",
    "X_train_pre_sin_Nan = X_train_preprocesado.dropna()\n",
    "y_train_sin_Nan = y_train.loc[X_train_pre_sin_Nan.index]\n",
    "X_val_pre_sin_Nan = X_val_preprocesado.dropna()\n",
    "y_val_sin_Nan = y_val.loc[X_val_pre_sin_Nan.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logmodel = LogisticRegression()\n",
    "\n",
    "logmodel.fit(X_train_pre_sin_Nan,y_train_sin_Nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_pre = logmodel.predict(X_val_pre_sin_Nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735de382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_val_sin_Nan,predictions_pre))\n",
    "\n",
    "print(confusion_matrix(y_val_sin_Nan,predictions_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3a8a5",
   "metadata": {},
   "source": [
    "Al simplificar los datos, la regresión logística se comporta mejor. Sin embargo, los resultados siguen siendo malos, F1 de 0.49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1ceefa",
   "metadata": {},
   "source": [
    "### Prueba con mayor control en el modelo. \n",
    "1. Balancear numero de muestras en las clases.\n",
    "2. Modificar umbral de decisión para predecir más 1 (inusuales). Estó mejorará el recall, a cambio de precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b975ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seguimos con el preprocesado que tenia ligeramente mejor comportamiento\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logmodel = LogisticRegression(class_weight='balanced') #asigna pesos automaticamente a las muestras de cada tipo de forma inversmanete proporcional a su frecuencia en el dataset. Así se compensa la falta de presencia de muestras tipo 1 (comportamiento inusual)\n",
    "\n",
    "logmodel.fit(X_train_pre_sin_Nan,y_train_sin_Nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3096891",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_balanceado = logmodel.predict(X_val_pre_sin_Nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf3ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_val_sin_Nan,predictions_balanceado))\n",
    "\n",
    "print(confusion_matrix(y_val_sin_Nan,predictions_balanceado))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e1bd3",
   "metadata": {},
   "source": [
    "mejora significativamente el rendimiento, pasamos a predecir de 13 a 1819 clases tipo 1 correctamente. La recall de la clase 1 aumenta de 0.09 a 0.71. Ahora el F1 score es de o.61"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b133be",
   "metadata": {},
   "source": [
    "Vamos a estudiar también la ROC y su AUC. Ayudará también a ver si merece la pena ajustar el umbral de decisión o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72901a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obtener probabilidades de la clase 1\n",
    "y_proba = logmodel.predict_proba(X_val_pre_sin_Nan)[:, 1]\n",
    "\n",
    "# Calcular curva ROC\n",
    "fpr, tpr, umbrales = roc_curve(y_val_sin_Nan, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Graficar\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener TPR, FPR y umbrales\n",
    "fpr, tpr, umbrales = roc_curve(y_val_sin_Nan, y_proba)\n",
    "\n",
    "# Método 1: Distancia al punto (0, 1)\n",
    "distancias = np.sqrt((fpr - 0)**2 + (tpr - 1)**2)\n",
    "mejor_idx = np.argmin(distancias)\n",
    "mejor_umbral_1 = umbrales[mejor_idx]\n",
    "\n",
    "# Método 2: Índice de Youden\n",
    "youden_index = tpr - fpr\n",
    "mejor_idx_2 = np.argmax(youden_index)\n",
    "mejor_umbral_2 = umbrales[mejor_idx_2]\n",
    "\n",
    "print(f\"Mejor umbral (distancia): {mejor_umbral_1:.3f}\")\n",
    "print(f\"Mejor umbral (Youden): {mejor_umbral_2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410eef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_custom = (y_proba > 0.54).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val_sin_Nan, y_pred_custom))\n",
    "print(confusion_matrix(y_val_sin_Nan, y_pred_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6d0a1f",
   "metadata": {},
   "source": [
    "Cómo se quiere maximizar el F1 nos quedamos con esta ultima versión. Modificando el umbral, estamos empeorando el recall de la clase 1, vamos a detectar menos porcentaje de comportamientos inusuales, pero vamos a tener menos falsos positivos (clasificados como inusual cuando son usuales). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d244313",
   "metadata": {},
   "source": [
    "Una última prueba para mejorar el comportamiento del modelo de regresión logistica será gestionar la multicolinealidad. Al principio, en el EDA, se vió como había variables muy correladas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf679aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Las que tenían mucha correlación eran 'maxUE_DL', 'maxUE_UL', 'maxUE_UL+DL'\n",
    "#tambien 'meanThr_DL' con 'PRBUsageDL'\n",
    "\n",
    "X_train_reduced = X_train_pre_sin_Nan.drop(['maxUE_DL','maxUE_UL','meanThr_DL'], axis=1)\n",
    "x_val_reduced = X_val_pre_sin_Nan.drop(['maxUE_DL','maxUE_UL','meanThr_DL'], axis=1)\n",
    "\n",
    "x_val_reduced.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seguimos con el preprocesado que tenia ligeramente mejor comportamiento\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logmodel = LogisticRegression(class_weight='balanced') #asigna pesos automaticamente a las muestras de cada tipo de forma inversmanete proporcional a su frecuencia en el dataset. Así se compensa la falta de presencia de muestras tipo 1 (comportamiento inusual)\n",
    "\n",
    "logmodel.fit(X_train_reduced,y_train_sin_Nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cfacc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_balanceado = logmodel.predict(x_val_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = logmodel.predict_proba(x_val_reduced)[:, 1]\n",
    "\n",
    "y_pred_custom = (y_proba > 0.54).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val_sin_Nan, y_pred_custom))\n",
    "print(confusion_matrix(y_val_sin_Nan, y_pred_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e4a45",
   "metadata": {},
   "source": [
    "No ha afectado eliminar las variables mas correladas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc82e9",
   "metadata": {},
   "source": [
    "## Modelo más complejo - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f399f605",
   "metadata": {},
   "source": [
    "voy a probar este modelo mas complejo para intentar mejorarar el F1. Si la relación entre target y variables es no lineal, este funcionará mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Modelo con parámetros iniciales\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',  \n",
    "    random_state=42,\n",
    "    n_jobs=-1 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1078f75",
   "metadata": {},
   "source": [
    "voy a probar con los datos sin preprocesar, al ser un modelo más complejo puede que capte bien esos patrones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feee6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar\n",
    "rf_model.fit(X_train_sin_Nan, y_train_sin_Nan)\n",
    "\n",
    "# Predecir probabilidades (para ajustar umbral después)\n",
    "y_proba_rf = rf_model.predict_proba(X_val_sin_Nan)[:, 1]\n",
    "\n",
    "# Predecir con umbral por defecto (0.5)\n",
    "y_pred_rf = rf_model.predict(X_val_sin_Nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfd61ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest - Resultados:\")\n",
    "print(classification_report(y_val_sin_Nan, y_pred_rf))\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(y_val_sin_Nan, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ee7d0",
   "metadata": {},
   "source": [
    "mejora considerablemente la calidad de las predicciones del modelo usando Random Forest. Veamos si es mejor sin outliers y distribuciones simetricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar\n",
    "rf_model.fit(X_train_pre_sin_Nan, y_train_sin_Nan)\n",
    "\n",
    "# Predecir probabilidades (para ajustar umbral después)\n",
    "y_proba_rf = rf_model.predict_proba(X_val_pre_sin_Nan)[:, 1]\n",
    "\n",
    "# Predecir con umbral por defecto (0.5)\n",
    "y_pred_rf = rf_model.predict(X_val_pre_sin_Nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e4c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest - Resultados:\")\n",
    "print(classification_report(y_val_sin_Nan, y_pred_rf))\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(y_val_sin_Nan, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c62fe4",
   "metadata": {},
   "source": [
    "mejora la precisón eliminando outliers. Lo última sería intentar ajustar hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7255a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir parámetros a optimizar\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Búsqueda grid con validación cruzada\n",
    "RF_grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',  # Optimizar para F1-score\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "RF_grid_search.fit(X_train_pre_sin_Nan, y_train_sin_Nan)\n",
    "\n",
    "# Mejores parámetros\n",
    "print(\"Mejores parámetros:\", RF_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f6a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir probabilidades (para ajustar umbral después)\n",
    "y_proba_rf = RF_grid_search.predict_proba(X_val_pre_sin_Nan)[:, 1]\n",
    "\n",
    "# Predecir con umbral por defecto (0.5)\n",
    "y_pred_rf = RF_grid_search.predict(X_val_pre_sin_Nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f751656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obtener probabilidades de la clase 1\n",
    "y_proba = RF_grid_search.predict_proba(X_val_pre_sin_Nan)[:, 1]\n",
    "\n",
    "# Calcular curva ROC\n",
    "fpr, tpr, umbrales = roc_curve(y_val_sin_Nan, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Graficar\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021a913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener TPR, FPR y umbrales\n",
    "fpr, tpr, umbrales = roc_curve(y_val_sin_Nan, y_proba)\n",
    "\n",
    "# Método 1: Distancia al punto (0, 1)\n",
    "distancias = np.sqrt((fpr - 0)**2 + (tpr - 1)**2)\n",
    "mejor_idx = np.argmin(distancias)\n",
    "mejor_umbral_1 = umbrales[mejor_idx]\n",
    "\n",
    "# Método 2: Índice de Youden\n",
    "youden_index = tpr - fpr\n",
    "mejor_idx_2 = np.argmax(youden_index)\n",
    "mejor_umbral_2 = umbrales[mejor_idx_2]\n",
    "\n",
    "print(f\"Mejor umbral (distancia): {mejor_umbral_1:.3f}\")\n",
    "print(f\"Mejor umbral (Youden): {mejor_umbral_2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68134fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_custom = (y_proba_rf > 0.389).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Random Forest - Resultados con umbral ajustado:\")\n",
    "print(classification_report(y_val_sin_Nan, y_pred_custom))\n",
    "print(\"Matriz de confusión con umbral ajustado:\")\n",
    "print(confusion_matrix(y_val_sin_Nan, y_pred_custom))\n",
    "\n",
    "print(\"Random Forest - Resultados:\")\n",
    "print(classification_report(y_val_sin_Nan, y_pred_rf))\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(y_val_sin_Nan, y_pred_rf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
